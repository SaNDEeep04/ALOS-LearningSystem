import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments
from datasets import load_dataset

# Load the tokenizer and model
tokenizer = T5Tokenizer.from_pretrained("t5-small")
model = T5ForConditionalGeneration.from_pretrained("t5-small")

# Load the dataset
dataset = load_dataset("squad")  # Replace "squad" with your dataset if using a custom one

# Preprocessing function to tokenize and prepare inputs/labels
def preprocess_function(examples):
    # Handle cases where answers might be a string or a list of strings
    if isinstance(examples["answers"], list):
        # If answers are lists of strings, process each element
        targets = [a["text"][0].strip() if isinstance(a["text"], list) and a["text"] else "" for a in examples["answers"]]
    else:
        # If answers are single strings, process directly
        targets = [examples["answers"].strip()]

    inputs = [q.strip() for q in examples["question"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")

    # Tokenize the targets (answers)
    labels = tokenizer(targets, max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

# Apply preprocessing
tokenized_datasets = dataset.map(preprocess_function, batched=True)

# Set up the training arguments
training_args = TrainingArguments(
    output_dir="./results",          # Directory to save results
    evaluation_strategy="epoch",    # Evaluate at the end of each epoch
    learning_rate=5e-5,             # Learning rate
    per_device_train_batch_size=8,  # Batch size for training
    per_device_eval_batch_size=8,   # Batch size for evaluation
    num_train_epochs=3,             # Number of training epochs
    weight_decay=0.01,              # Weight decay
    save_strategy="epoch",          # Save checkpoint after every epoch
    logging_dir="./logs",           # Directory to save logs
    logging_steps=10,               # Log every 10 steps
    push_to_hub=False,              # Do not push to Hugging Face Hub
)

# Create the Trainer instance
trainer = Trainer(
    model=model,                         # The model to train
    args=training_args,                  # Training arguments
    train_dataset=tokenized_datasets["train"],  # Training dataset
    eval_dataset=tokenized_datasets["validation"],  # Evaluation dataset
    tokenizer=tokenizer,                 # Tokenizer for data processing
)

# Train the model
trainer.train()

# Save the model and tokenizer
model.save_pretrained("./educational_chatbot")
tokenizer.save_pretrained("./educational_chatbot")

print("Training complete. Model saved!")

